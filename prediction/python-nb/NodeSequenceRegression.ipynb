{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome value prediction from Behaviour Change Data\n",
    "\n",
    "In this notebook, we set up a regression/classification pipeline to predict either the outcome value\n",
    "directly or an interval in which this value may fall into, the former being `regression` and the latter being `multi-class classification`.\n",
    "\n",
    "Before getting started with this notebook, let's have a look at the format of the data files which this program expects. The data is specified as tsv (tab separated values) and is generated by running the following command from the HBCP project root directory.\n",
    "\n",
    "```\n",
    "mvn exec:java@svmreg -Dexec.args=\"true\"\n",
    "```\n",
    "The above command generates the train and the test files located at the directory `prediction/sentences/`.\n",
    "Each line of the data files (train and test) looks like the following:\n",
    "```\n",
    "C:5579097:35.7 C:5594106:16.1 I:3674268:1 C:5579728:30.6 I:3674248:1 C:5579118:22 C:5579689:14.6 C:5579088:44.5 C:5579711:80.6 C:5580203:29.4 O:4087178:abstinence C:5594105:19.3 O:4087186:cotinine C:5580200:35.7 O:4087187:2 C:5579096:58.8 I:3675703:1 C:5580204:5.9 I:3675698:1 C:5579663:22 C:5579083:29.4 O:4087191:6 I:3673288:1 O:4087172:1 I:3674264:1 I:3675717:1 C:5580216:0 \t2.8\n",
    "```\n",
    "\n",
    "Each token represents a `:`-separated `<attribute-type>:<attribute-id>:<value>` combination, where the attribute type is one of `{C, I, O}` (contextual, intervention or outcome qualifier) feature, an attribute-id is a unique integer and a value is the textual representation of an instance of this feature.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccessary imports and global variables\n",
    "import sys, getopt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from numpy.random import seed\n",
    "#from tensorflow import set_random_seed\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras import layers\n",
    "\n",
    "seed(110781)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(110781)\n",
    "\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "PATTERN = re.compile(\"(?<![0-9])-?[0-9]*\\.?[0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features\n",
    "Use the gensim library to extend the vocabulary to a set of `words` from the PubMed literature. These words are then used to augment a node vector with the sum of the constituent word vectors from the `value` of a node, e.g. an intervention of `Goal Setting` may contain as its value the text `encouraging patients to set a date for quitting`. The vectors for these words are then aggregated and added as additional dimensions of a node vector representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputHelper(object):\n",
    "    emb_dim = 0\n",
    "    pre_emb = dict() # word--> vec\n",
    "    vocab_size = 0\n",
    "    tokenizer = None\n",
    "    embedding_matrix = None\n",
    "    \n",
    "    def cleanText(self, s):\n",
    "        s = re.sub(r\"[^\\x00-\\x7F]+\",\" \", s)\n",
    "        s = re.sub(r'[\\~\\!\\`\\^\\*\\{\\}\\[\\]\\#\\<\\>\\?\\+\\=\\-\\_\\(\\)]+',\"\",s)\n",
    "        s = re.sub(r'( [0-9,\\.]+)',r\"\\1 \", s)\n",
    "        s = re.sub(r'\\$',\" $ \", s)\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        return s.lower()\n",
    "\n",
    "    #the tokenizer needs to be trained on the pre-trained node vectors\n",
    "    #join the names of the nodes in a string so that tokenizer could be fit on it\n",
    "    def getAllNodes(self, emb_path):\n",
    "        print(\"Collecting node names...\")\n",
    "        line_count = 0        \n",
    "        node_names = []\n",
    "        for line in open(emb_path):\n",
    "            l = line.strip().split()\n",
    "            if (line_count > 0):\n",
    "                node_names.append(l[0])\n",
    "            \n",
    "            line_count = line_count + 1\n",
    "        \n",
    "        self.vocab_size = line_count # includes the +1\n",
    "        print(\"Collected node names...\")\n",
    "        return node_names\n",
    "\n",
    "    # call convertWordsToIds first followed by loadW2V\n",
    "    def convertWordsToIds(self, emb_path):\n",
    "        allNodeNames = self.getAllNodes(emb_path)\n",
    "        \n",
    "        print (\"Converting words to ids...\")\n",
    "        # Map words to ids        \n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, filters=[], lower=False, split=\" \")\n",
    "        self.tokenizer.fit_on_texts(allNodeNames)\n",
    "        print (\"Finished converting words to ids...\")\n",
    "    \n",
    "    # Assumes that the tokenizer already has been fit on some text (for the time being the node vec names)\n",
    "    def loadW2V(self, emb_path):\n",
    "        print(\"Loading W2V data...\")\n",
    "        line_count = 0        \n",
    "        \n",
    "        for line in open(emb_path):\n",
    "            l = line.strip().split()\n",
    "            if (line_count == 0): # the first line -- supposed to be <vocab-size> <dimension>\n",
    "                self.emb_dim = int(l[1])\n",
    "                self.embedding_matrix = np.zeros((self.vocab_size, self.emb_dim))\n",
    "            else:\n",
    "                st = l[0]\n",
    "                self.pre_emb[st] = np.asarray(l[1:]) # rest goes as the vector components\n",
    "                if st in self.tokenizer.word_index:\n",
    "                    idx = self.tokenizer.word_index[st]\n",
    "                    self.embedding_matrix[idx] = np.array(l[1:], dtype=np.float32)[:self.emb_dim]\n",
    "                else:\n",
    "                    print (\"Word '{}' not found in vocabulary..\".format(st))\n",
    "                \n",
    "            line_count = line_count + 1\n",
    "            \n",
    "        print(\"loaded word2vec for {} nodes\".format(len(self.pre_emb)))\n",
    "    \n",
    "    # Load the data as two matrices - X and Y\n",
    "    def getTsvData(self, filepath):\n",
    "        print(\"Loading data from \" + filepath)\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        # positive samples from file\n",
    "        for line in open(filepath):\n",
    "            l = line.strip().split(\"\\t\")            \n",
    "            y.append(l[1])\n",
    "            words = l[0].split(\" \")\n",
    "            x.append(words)\n",
    "            #for w in words:\n",
    "            #    x.append(w)\n",
    "            \n",
    "        return np.asarray(x), np.asarray(y)\n",
    "    \n",
    "    # Build sequences from each data instance\n",
    "    def getSequenceData(self, tsvDataPath, numClasses=0, seed=123456):\n",
    "        x_text, y = self.getTsvData(tsvDataPath)\n",
    "        \n",
    "        # Convert each sentence (node name sequence) to a sequence of integer ids\n",
    "        x = self.tokenizer.texts_to_sequences(x_text)\n",
    "        #print (x)\n",
    "        \n",
    "        if (numClasses > 0):\n",
    "            y = self.categorizeOutputs(y, numClasses)\n",
    "            #print (y)\n",
    "        \n",
    "        return x, np.asarray(y)\n",
    "    \n",
    "    def categorizeOutputs(self, y, numClasses):\n",
    "        y_scaled = []\n",
    "        for y_i in y:\n",
    "            #y_i = mapToUniformlySpacedIntervals(y_i, numClasses)\n",
    "            y_i = mapToNonUniformlySpacedIntervals(y_i)\n",
    "            y_scaled.append(y_i)\n",
    "                \n",
    "        return y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToUniformlySpacedIntervals(y_i, numClasses):\n",
    "    MAX = 100\n",
    "    DELTA = MAX/numClasses\n",
    "    y_i = float(y_i)\n",
    "    y_i = int(y_i/DELTA)\n",
    "    return y_i\n",
    "    \n",
    "def mapToNonUniformlySpacedIntervals(y_i):\n",
    "    #[0,5] [5,10] [10, 15] [15,20] [20,30] [30,50] [50,100]\n",
    "    y_i = float(y_i)\n",
    "    if y_i < 5:\n",
    "        y_i = 0\n",
    "    elif y_i>=5 and y_i<10:\n",
    "        y_i = 1\n",
    "    elif y_i>=10 and y_i<15:\n",
    "        y_i = 2\n",
    "    elif y_i>=15 and y_i<20:\n",
    "        y_i = 3\n",
    "    elif y_i>=20 and y_i<30:\n",
    "        y_i = 4\n",
    "    elif y_i>=30 and y_i<50:\n",
    "        y_i = 5\n",
    "    else:\n",
    "        y_i = 6\n",
    "\n",
    "    return y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_feature(in_fn, out_fn):\n",
    "    \"\"\"Read (text) file of (dense) vectors and add a final value to the vector for the actual value of the node.\n",
    "    This allows to represent the distance between nodes. Normalize this feature between -1 and 1.\n",
    "    For numerical -1 is the minimum value in the range, 1 the max.\n",
    "    For BCT, +1 is the presence, -1 is the absence\n",
    "    For categorical we create ranges bet. -1 and 1.\n",
    "    For other, pick some random number close to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    print (\"Writing appended vec file at %s\" %(out_fn))\n",
    "    random.seed(123)\n",
    "    att_values, type_att = collect_attribute_value_maps(in_fn)\n",
    "    numeric_atts = infer_numerical_attributes(att_values, type_att)\n",
    "\n",
    "    att_maxes, att_mins = get_att_max_min(att_values, numeric_atts)  # max/min used for normalization\n",
    "\n",
    "    # debug -- print maxes and mins\n",
    "    print(\"There are %d numeric attributes.\" % len(numeric_atts))\n",
    "    for num_att_id in numeric_atts:\n",
    "        print(\"Numeric att: %s -- Min: %f ; Max: %f\" % (num_att_id, att_mins[num_att_id], att_maxes[num_att_id]))\n",
    "\n",
    "    # go through the file again and add 'normalized' values\n",
    "    with open(in_fn) as f:\n",
    "        with open(out_fn, 'w') as f_out:\n",
    "            for line in f:\n",
    "                cols = line.split()\n",
    "                if len(cols) == 2:  # first line\n",
    "                    f_out.write(line)\n",
    "                    continue\n",
    "                prefix, att_id, val = cols[0].split(':', 2)\n",
    "                # BCTs stay the same\n",
    "                if prefix == 'I':\n",
    "                    norm_val = val\n",
    "                # numerical attributes get normalized\n",
    "                elif att_id in numeric_atts:\n",
    "                    match = PATTERN.search(val)\n",
    "                    if match is not None:\n",
    "                        num = float(match.group(0))\n",
    "                        # max-min normalization\n",
    "                        if att_maxes[att_id] == att_mins[att_id]:\n",
    "                            norm_val = \"1\"\n",
    "                        else:\n",
    "                            norm_num = 2 * ((num - att_mins[att_id]) / (att_maxes[att_id] - att_mins[att_id])) - 1\n",
    "                            norm_val = str(norm_num)\n",
    "                    else:\n",
    "                        norm_val = \"%f\" % random.gauss(0, 0.001)\n",
    "                # TODO not keeping track of categorical attributes yet\n",
    "                # remaining attributes will get a random value close to zero (not sure what else to do with them)\n",
    "                else:\n",
    "                    norm_val = \"%f\" % random.gauss(0, 0.001)\n",
    "                # f_out.write(cols[0] + '\\t' + norm_val + '\\n')\n",
    "                f_out.write('{0} {1}\\n'.format(line.strip(), norm_val))\n",
    "\n",
    "\n",
    "def get_att_max_min(att_values, numeric_atts):\n",
    "    # normalize numeric attributes\n",
    "    att_maxes = {}\n",
    "    att_mins = {}\n",
    "    for num_att_id in numeric_atts:\n",
    "        # get max and min\n",
    "        nums = []\n",
    "        for val in att_values[num_att_id]:\n",
    "            # does val have a number\n",
    "            match = PATTERN.search(val)\n",
    "            if match is not None:\n",
    "                num = float(match.group(0))\n",
    "                nums.append(num)\n",
    "        att_mins[num_att_id] = min(nums)\n",
    "        att_maxes[num_att_id] = max(nums)\n",
    "    return att_maxes, att_mins\n",
    "\n",
    "\n",
    "def infer_numerical_attributes(att_values, type_att):\n",
    "    # check if attribute is numerical (use logic from Martin's Java code)\n",
    "    numeric_atts = []\n",
    "    for att_id, vals in att_values.items():\n",
    "        if att_id in type_att['I']:\n",
    "            continue  # interventions will all be '1'\n",
    "        num_val = 0\n",
    "        for val in vals:\n",
    "            # does val have a number\n",
    "            match = PATTERN.search(val)\n",
    "            if match is not None:\n",
    "                num_val += 1\n",
    "        # if 80% or more have numbers, then consider numeric\n",
    "        if num_val / len(vals) >= 0.8:\n",
    "            numeric_atts.append(att_id)\n",
    "    return numeric_atts\n",
    "\n",
    "\n",
    "def collect_attribute_value_maps(fn):\n",
    "    att_values = {}\n",
    "    type_att = {'C': set(), 'I': set(), 'O': set(), 'V': set()}\n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            cols = line.split()\n",
    "            if len(cols) == 2:  # first line, skip\n",
    "                continue\n",
    "            prefix, att_id, val = cols[0].split(':', 2)\n",
    "            type_att[prefix].add(att_id)\n",
    "            if att_id in att_values:\n",
    "                att_values[att_id].append(val)\n",
    "            else:\n",
    "                att_values[att_id] = [val]\n",
    "    print(\"There are %d attributes.\" % len(att_values.keys()))\n",
    "    print(\"There are %d interventions.\" % len(type_att['I']))\n",
    "    return att_values, type_att\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the global parameters. Set `NUM_CLASSES` to `0` if you want to run the regression flow, otherwise set this to the number of classes (outcome value ranges).\n",
    "\n",
    "The function `convertWordsToIds` converts each word (e.g. `C:5579097:35.7`) into an id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets and embedded vectors\n",
    "def loadData(inpH, emb_file, train_file, test_file, numClasses=0):\n",
    "    \n",
    "    #Load the training and the test sets\n",
    "    #Load the text as a sequence of inputs\n",
    "    \n",
    "    x_train, y_train = inpH.getSequenceData(train_file, numClasses)\n",
    "    x_test, y_test = inpH.getSequenceData(test_file, numClasses)\n",
    "\n",
    "    #First few sequences\n",
    "    for i in range(4):\n",
    "        print (\"x[{}][:5]..., y[{}] = {}, {}\".format(i, i, x_train[i][:5], y_train[i]))    \n",
    "        \n",
    "    if (numClasses > 0):    \n",
    "        encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "        \n",
    "        #y_all = np.vstack((y_train, y_test))\n",
    "        y_all = np.append(y_train, y_test)\n",
    "\n",
    "        encoder.fit(y_all.reshape(-1,1))\n",
    "\n",
    "        y_train = encoder.transform(y_train.reshape(-1, 1))\n",
    "        y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        for i in range(2):\n",
    "            print (\"y_train[{}] = {}\".format(i, y_train[i]))\n",
    "            print (\"y_test[{}] = {}\".format(i, y_test[i]))\n",
    "            \n",
    "    #Load the word vectors\n",
    "    inpH.loadW2V(emb_file)\n",
    "    \n",
    "    #Print the loaded words\n",
    "    nwords=0\n",
    "    for w in inpH.pre_emb:\n",
    "        print (\"Dimension of vectors: {}\".format(inpH.pre_emb[w].shape))\n",
    "        print (\"{} {}\".format(w, inpH.pre_emb[w][0:5]))\n",
    "        nwords = nwords+1\n",
    "        if (nwords >= 2): break\n",
    "\n",
    "    print (\"vocab size: {}\".format(inpH.vocab_size))\n",
    "    print (\"emb-matrix: {}...\".format(inpH.embedding_matrix[1][:5]))\n",
    "    print (inpH.embedding_matrix.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER='adam'\n",
    "ACTIVATION='relu'\n",
    "EPOCHS=20\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def trainRegression(inpH, x_train, y_train, x_test, y_test, batch_size, hidden_layer_dim=20, num_classes=0, epochs=EPOCHS, maxlen=300):\n",
    "    if (num_classes > 0):\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        eval_metrics = ['accuracy']\n",
    "        activation_fn = 'softmax'\n",
    "        output_dim = num_classes\n",
    "    else:\n",
    "        loss_fn = rmse\n",
    "        eval_metrics = [rmse]\n",
    "        activation_fn = 'linear'\n",
    "        output_dim = 1\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=inpH.vocab_size, \n",
    "                               output_dim=inpH.emb_dim, \n",
    "                               input_length=maxlen,\n",
    "                               weights=[inpH.embedding_matrix],\n",
    "                               trainable=False))\n",
    "    #model.add(layers.Flatten())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Dropout(0.2))    \n",
    "    model.add(layers.Dense(30, activation=ACTIVATION))\n",
    "    model.add(layers.Dense(output_dim, activation=activation_fn, name='output_vals'))\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss = loss_fn,\n",
    "                  metrics=eval_metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    print (\"Training model...\")\n",
    "    history = model.fit(x_train, y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        #validation_split=0.1,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=True)\n",
    "    if (num_classes > 0):\n",
    "        print(\"Cross-entropy loss: {:.4f}, Accuracy: {:.4f}\".format(loss, accuracy))\n",
    "    else:\n",
    "        print(\"Loss: {:.4f}, RMSE: {:.4f}\".format(loss, accuracy))    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRegression(model, x_test, y_test, res_file, num_classes=0):\n",
    "    #Print each prediction    \n",
    "    y_preds = model.predict(x_test)\n",
    "    \n",
    "    f = open(res_file, \"w\")\n",
    "    for i in range(y_preds.shape[0]):\n",
    "        if (num_classes > 0):\n",
    "            f.write (\"{:d}\\t{:d}\\n\".format(np.argmax(y_test[i]), np.argmax(y_preds[i])))\n",
    "        else:\n",
    "            f.write (\"{}\\t{:.4f}\\n\".format(y_test[i], y_preds[i][0]))\n",
    "\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    NUM_CLASSES = 7 # set this to 0 for regression and a positive value for classification\n",
    "    TRAIN_FILE = \"../sentences/train.tsv\"\n",
    "    TEST_FILE = \"../sentences/test.tsv\"\n",
    "    TO_ADD_VALUE = 0\n",
    "    RES_FILE = \"predictions.txt\"\n",
    "    #For embedding file with concatenated word features use this\n",
    "    EMB_FILE = \"../graphs/nodevecs/nodes_and_words.vec\"\n",
    "    #For node vectors only, use this - \n",
    "    #EMB_FILE = \"../graphs/nodevecs/refVecs.vec\"\n",
    "    MAXLEN=50\n",
    "    \n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"ha:i:e:o:n:\", [\"appendnumeric=\", \"trainfile=\", \"testfile=\", \"resfile=\", \"nodevecs=\"])\n",
    "    \n",
    "        for opt, arg in opts:\n",
    "            if opt == '-h':\n",
    "                print ('NodeSequenceRegression.py -i <trainfile> -o <testfile> -a -o <resfile> -n <nodevecs>')\n",
    "                sys.exit()\n",
    "            elif opt in (\"-i\", \"--trainfile\"):\n",
    "                TRAIN_FILE = arg\n",
    "            elif opt in (\"-e\", \"--testfile\"):\n",
    "                TEST_FILE = arg\n",
    "            elif opt in (\"-a\", \"--appendnumeric\"):\n",
    "                TO_ADD_VALUE = 1\n",
    "            elif opt in (\"-n\", \"--nodevecs\"):\n",
    "                EMB_FILE = arg\n",
    "            elif opt in (\"-o\", \"--resfile\"):\n",
    "                RES_FILE = arg\n",
    "                \n",
    "    except getopt.GetoptError:\n",
    "        print ('usage: NodeSequenceRegression.py -i <trainfile> -o <testfile> -a -o <resfile> -n <nodevecs>')\n",
    "            \n",
    "    print (\"Training file: %s\" % (TRAIN_FILE))\n",
    "    print (\"Test file: %s\" % (TEST_FILE))\n",
    "    print (\"Append numbers: %d\" % (TO_ADD_VALUE))\n",
    "    print (\"Res file: %s\" % (RES_FILE))\n",
    "    print (\"Emb file: %s\" % (EMB_FILE))\n",
    "\n",
    "    inpH = InputHelper()\n",
    "    inpH.convertWordsToIds(EMB_FILE)\n",
    "    \n",
    "    VAL_ADDED_EMBFILE = EMB_FILE\n",
    "    embfileDir = os.path.dirname(os.path.realpath(EMB_FILE))\n",
    "\n",
    "    if (TO_ADD_VALUE == 1):\n",
    "        VAL_ADDED_EMBFILE = embfileDir + '/ndvecswithvals.vec' \n",
    "        add_value_feature(EMB_FILE, VAL_ADDED_EMBFILE)\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = loadData(inpH, VAL_ADDED_EMBFILE, TRAIN_FILE, TEST_FILE, numClasses=NUM_CLASSES)\n",
    "    \n",
    "    BATCH_SIZE = int(len(x_train)/20) # 5% of the training set size\n",
    "    x_train = pad_sequences(x_train, padding='post', maxlen=MAXLEN)\n",
    "    x_test = pad_sequences(x_test, padding='post', maxlen=MAXLEN)\n",
    "    \n",
    "    model = trainRegression(inpH, x_train, y_train, x_test, y_test, BATCH_SIZE, maxlen=MAXLEN, num_classes=NUM_CLASSES)\n",
    "    testRegression(model, x_test, y_test, RES_FILE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: NodeSequenceRegression.py -i <trainfile> -o <testfile> -a -o <resfile> -n <nodevecs>\n",
      "Training file: ../sentences/train.tsv\n",
      "Test file: ../sentences/test.tsv\n",
      "Append numbers: 0\n",
      "Res file: predictions.txt\n",
      "Emb file: ../graphs/nodevecs/nodes_and_words.vec\n",
      "Collecting node names...\n",
      "Collected node names...\n",
      "Converting words to ids...\n",
      "Finished converting words to ids...\n",
      "Loading data from ../sentences/train.tsv\n",
      "Loading data from ../sentences/test.tsv\n",
      "x[0][:5]..., y[0] = [5329, 9500, 6392, 8525, 12649], 4\n",
      "x[1][:5]..., y[1] = [11266, 2941, 4327, 4131, 6732], 3\n",
      "x[2][:5]..., y[2] = [4327, 3633, 13451, 10676, 1644], 1\n",
      "x[3][:5]..., y[3] = [5389, 6073, 4022, 7480, 2941], 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-431-0af32a89de4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#main('-i ../sentences/train.tsv -e ../sentences/test.tsv -a -o predictions.txt -n ../graphs/nodevecs/refVecs.vec')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-430-cf6397b52c35>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0madd_value_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMB_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_ADDED_EMBFILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_ADDED_EMBFILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 5% of the training set size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-427-441cb5aa16b9>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(inpH, emb_file, train_file, test_file, numClasses)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mX_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     45\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    584\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 586\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "    #main('-i ../sentences/train.tsv -e ../sentences/test.tsv -a -o predictions.txt -n ../graphs/nodevecs/refVecs.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
