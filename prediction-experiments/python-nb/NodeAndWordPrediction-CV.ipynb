{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome value prediction from Behaviour Change Data\n",
    "\n",
    "In this notebook, we set up a regression/classification pipeline to predict either the outcome value\n",
    "directly or an interval in which this value may fall into, the former being `regression` and the latter being `multi-class classification`.\n",
    "\n",
    "Before getting started with this notebook, let's have a look at the format of the data files which this program expects. The data is specified as tsv (tab separated values) and is generated by running the following command from the HBCP project root directory.\n",
    "\n",
    "```\n",
    "mvn exec:java@svmreg -Dexec.args=\"true\"\n",
    "```\n",
    "The above command generates the train and the test files located at the directory `prediction/sentences/`.\n",
    "Each line of the data files (train and test) looks like the following:\n",
    "```\n",
    "C:5579097:35.7 C:5594106:16.1 I:3674268:1 C:5579728:30.6 I:3674248:1 C:5579118:22 C:5579689:14.6 C:5579088:44.5 C:5579711:80.6 C:5580203:29.4 O:4087178:abstinence C:5594105:19.3 O:4087186:cotinine C:5580200:35.7 O:4087187:2 C:5579096:58.8 I:3675703:1 C:5580204:5.9 I:3675698:1 C:5579663:22 C:5579083:29.4 O:4087191:6 I:3673288:1 O:4087172:1 I:3674264:1 I:3675717:1 C:5580216:0 \t2.8\n",
    "```\n",
    "\n",
    "Each token represents a `:`-separated `<attribute-type>:<attribute-id>:<value>` combination, where the attribute type is one of `{C, I, O}` (contextual, intervention or outcome qualifier) feature, an attribute-id is a unique integer and a value is the textual representation of an instance of this feature.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccessary imports and global variables\n",
    "import sys, getopt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from numpy.random import seed\n",
    "#from tensorflow import set_random_seed\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras import layers\n",
    "\n",
    "SEED = 110781 \n",
    "seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "PATTERN = re.compile(\"(?<![0-9])-?[0-9]*\\.?[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_feature(in_fn, out_fn):\n",
    "    \"\"\"Read (text) file of (dense) vectors and add a final value to the vector for the actual value of the node.\n",
    "    This allows to represent the distance between nodes. Normalize this feature between -1 and 1.\n",
    "    For numerical -1 is the minimum value in the range, 1 the max.\n",
    "    For BCT, +1 is the presence, -1 is the absence\n",
    "    For categorical we create ranges bet. -1 and 1.\n",
    "    For other, pick some random number close to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    print (\"Writing appended vec file at %s\" %(out_fn))\n",
    "    random.seed(123)\n",
    "    att_values, type_att = collect_attribute_value_maps(in_fn)\n",
    "    numeric_atts = infer_numerical_attributes(att_values, type_att)\n",
    "\n",
    "    att_maxes, att_mins = get_att_max_min(att_values, numeric_atts)  # max/min used for normalization\n",
    "\n",
    "    # debug -- print maxes and mins\n",
    "    print(\"There are %d numeric attributes.\" % len(numeric_atts))\n",
    "    for num_att_id in numeric_atts:\n",
    "        print(\"Numeric att: %s -- Min: %f ; Max: %f\" % (num_att_id, att_mins[num_att_id], att_maxes[num_att_id]))\n",
    "\n",
    "    # go through the file again and add 'normalized' values\n",
    "    with open(in_fn) as f:\n",
    "        with open(out_fn, 'w') as f_out:\n",
    "            for line in f:\n",
    "                cols = line.split()\n",
    "                if len(cols) == 2:  # first line\n",
    "                    f_out.write(line)\n",
    "                    continue\n",
    "                prefix, att_id, val = cols[0].split(':', 2)\n",
    "                # BCTs stay the same\n",
    "                if prefix == 'I':\n",
    "                    norm_val = val\n",
    "                # numerical attributes get normalized\n",
    "                elif att_id in numeric_atts:\n",
    "                    match = PATTERN.search(val)\n",
    "                    if match is not None:\n",
    "                        num = float(match.group(0))\n",
    "                        # max-min normalization\n",
    "                        if att_maxes[att_id] == att_mins[att_id]:\n",
    "                            norm_val = \"1\"\n",
    "                        else:\n",
    "                            norm_num = 2 * ((num - att_mins[att_id]) / (att_maxes[att_id] - att_mins[att_id])) - 1\n",
    "                            norm_val = str(norm_num)\n",
    "                    else:\n",
    "                        norm_val = \"%f\" % random.gauss(0, 0.001)\n",
    "                # TODO not keeping track of categorical attributes yet\n",
    "                # remaining attributes will get a random value close to zero (not sure what else to do with them)\n",
    "                else:\n",
    "                    norm_val = \"%f\" % random.gauss(0, 0.001)\n",
    "                # f_out.write(cols[0] + '\\t' + norm_val + '\\n')\n",
    "                f_out.write('{0} {1}\\n'.format(line.strip(), norm_val))\n",
    "\n",
    "\n",
    "def get_att_max_min(att_values, numeric_atts):\n",
    "    # normalize numeric attributes\n",
    "    att_maxes = {}\n",
    "    att_mins = {}\n",
    "    for num_att_id in numeric_atts:\n",
    "        # get max and min\n",
    "        nums = []\n",
    "        for val in att_values[num_att_id]:\n",
    "            # does val have a number\n",
    "            match = PATTERN.search(val)\n",
    "            if match is not None:\n",
    "                num = float(match.group(0))\n",
    "                nums.append(num)\n",
    "        att_mins[num_att_id] = min(nums)\n",
    "        att_maxes[num_att_id] = max(nums)\n",
    "    return att_maxes, att_mins\n",
    "\n",
    "\n",
    "def infer_numerical_attributes(att_values, type_att):\n",
    "    # check if attribute is numerical (use logic from Martin's Java code)\n",
    "    numeric_atts = []\n",
    "    for att_id, vals in att_values.items():\n",
    "        if att_id in type_att['I']:\n",
    "            continue  # interventions will all be '1'\n",
    "        num_val = 0\n",
    "        for val in vals:\n",
    "            # does val have a number\n",
    "            match = PATTERN.search(val)\n",
    "            if match is not None:\n",
    "                num_val += 1\n",
    "        # if 80% or more have numbers, then consider numeric\n",
    "        if num_val / len(vals) >= 0.8:\n",
    "            numeric_atts.append(att_id)\n",
    "    return numeric_atts\n",
    "\n",
    "\n",
    "def collect_attribute_value_maps(fn):\n",
    "    att_values = {}\n",
    "    type_att = {'C': set(), 'I': set(), 'O': set(), 'V': set()}\n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            cols = line.split()\n",
    "            if len(cols) == 2:  # first line, skip\n",
    "                continue\n",
    "            prefix, att_id, val = cols[0].split(':', 2)\n",
    "            type_att[prefix].add(att_id)\n",
    "            if att_id in att_values:\n",
    "                att_values[att_id].append(val)\n",
    "            else:\n",
    "                att_values[att_id] = [val]\n",
    "    print(\"There are %d attributes.\" % len(att_values.keys()))\n",
    "    print(\"There are %d interventions.\" % len(type_att['I']))\n",
    "    return att_values, type_att\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features\n",
    "Use the gensim library to extend the vocabulary to a set of `words` from the PubMed literature. These words are then used to augment a node vector with the sum of the constituent word vectors from the `value` of a node, e.g. an intervention of `Goal Setting` may contain as its value the text `encouraging patients to set a date for quitting`. The vectors for these words are then aggregated and added as additional dimensions of a node vector representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToUniformlySpacedIntervals(y_i, numClasses):\n",
    "    MAX = 100\n",
    "    DELTA = MAX/numClasses\n",
    "    y_i = float(y_i)\n",
    "    y_i = int(y_i/DELTA)\n",
    "    return y_i\n",
    "    \n",
    "def mapToNonUniformlySpacedIntervals(y_i):\n",
    "    #[0,5] [5,10] [10, 15] [15,20] [20,30] [30,50] [50,100]\n",
    "    y_i = float(y_i)\n",
    "    if y_i < 5:\n",
    "        y_i = 0\n",
    "    elif y_i>=5 and y_i<10:\n",
    "        y_i = 1\n",
    "    elif y_i>=10 and y_i<15:\n",
    "        y_i = 2\n",
    "    elif y_i>=15 and y_i<20:\n",
    "        y_i = 3\n",
    "    elif y_i>=20 and y_i<30:\n",
    "        y_i = 4\n",
    "    elif y_i>=30 and y_i<50:\n",
    "        y_i = 5\n",
    "    else:\n",
    "        y_i = 6\n",
    "\n",
    "    return y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputHelper(object):\n",
    "    emb_dim = 0\n",
    "    pre_emb = dict() # word--> vec\n",
    "    vocab_size = 0\n",
    "    tokenizer = None\n",
    "    embedding_matrix = None\n",
    "    \n",
    "    def cleanText(self, s):\n",
    "        s = re.sub(r\"[^\\x00-\\x7F]+\",\" \", s)\n",
    "        s = re.sub(r'[\\~\\!\\`\\^\\*\\{\\}\\[\\]\\#\\<\\>\\?\\+\\=\\-\\_\\(\\)]+',\"\",s)\n",
    "        s = re.sub(r'( [0-9,\\.]+)',r\"\\1 \", s)\n",
    "        s = re.sub(r'\\$',\" $ \", s)\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        return s.lower()\n",
    "\n",
    "    #the tokenizer needs to be trained on the pre-trained node vectors\n",
    "    #join the names of the nodes in a string so that tokenizer could be fit on it\n",
    "    def getAllNodes(self, emb_path):\n",
    "        print(\"Collecting node names...\")\n",
    "        line_count = 0        \n",
    "        node_names = []\n",
    "        for line in open(emb_path):\n",
    "            l = line.strip().split()\n",
    "            if (line_count > 0):\n",
    "                node_names.append(l[0])\n",
    "            \n",
    "            line_count = line_count + 1\n",
    "        \n",
    "        self.vocab_size = line_count # includes the +1\n",
    "        print(\"Collected node names...\")\n",
    "        return node_names\n",
    "\n",
    "    # call convertWordsToIds first followed by loadW2V\n",
    "    def convertWordsToIds(self, emb_path):\n",
    "        allNodeNames = self.getAllNodes(emb_path)\n",
    "        \n",
    "        print (\"Converting words to ids...\")\n",
    "        # Map words to ids        \n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, filters=[], lower=False, split=\" \")\n",
    "        self.tokenizer.fit_on_texts(allNodeNames)\n",
    "        print (\"Finished converting words to ids...\")\n",
    "    \n",
    "    # Assumes that the tokenizer already has been fit on some text (for the time being the node vec names)\n",
    "    def loadW2V(self, emb_path):\n",
    "        print(\"Loading W2V data...\")\n",
    "        line_count = 0        \n",
    "        \n",
    "        for line in open(emb_path):\n",
    "            l = line.strip().split()\n",
    "            if (line_count == 0): # the first line -- supposed to be <vocab-size> <dimension>\n",
    "                self.emb_dim = int(l[1])\n",
    "                self.embedding_matrix = np.zeros((self.vocab_size, self.emb_dim))\n",
    "            else:\n",
    "                try:\n",
    "                    st = l[0]\n",
    "                    self.pre_emb[st] = np.asarray(l[1:]) # rest goes as the vector components\n",
    "                    if st in self.tokenizer.word_index:\n",
    "                        idx = self.tokenizer.word_index[st]\n",
    "                        self.embedding_matrix[idx] = np.array(l[1:], dtype=np.float32)[:self.emb_dim]\n",
    "                    else:\n",
    "                        print (\"Word '{}' not found in vocabulary..\".format(st))\n",
    "                except ValueError:\n",
    "                    print ('Line {} is corrupt!'.format(line_count))\n",
    "                \n",
    "            line_count = line_count + 1\n",
    "            \n",
    "        print(\"loaded word2vec for {} nodes\".format(len(self.pre_emb)))\n",
    "    \n",
    "    # Load the data as two matrices - X and Y\n",
    "    def getTsvData(self, filepath):\n",
    "        print(\"Loading data from \" + filepath)\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        # positive samples from file\n",
    "        for line in open(filepath):\n",
    "            l = line.strip().split(\"\\t\")            \n",
    "            y.append(l[1])\n",
    "            words = l[0].split(\" \")\n",
    "            x.append(words)\n",
    "            #for w in words:\n",
    "            #    x.append(w)\n",
    "            \n",
    "        return np.asarray(x), np.asarray(y)\n",
    "\n",
    "    #Load data from tsv file with fold info\n",
    "    def loadDataWithFolds(self, data_file, numClasses=0):\n",
    "        x, y = self.getSequenceData(data_file, numClasses)\n",
    "        return x, y\n",
    "    \n",
    "    # Build sequences from each data instance\n",
    "    def getSequenceData(self, tsvDataPath, numClasses=0):\n",
    "        x_text, y = self.getTsvData(tsvDataPath)\n",
    "        \n",
    "        # Convert each sentence (node name sequence) to a sequence of integer ids\n",
    "        x = self.tokenizer.texts_to_sequences(x_text)\n",
    "        #print (x)\n",
    "        \n",
    "        if (numClasses > 0):\n",
    "            y = self.categorizeOutputs(y, numClasses)\n",
    "        \n",
    "        return x, np.asarray(y)\n",
    "    \n",
    "    def categorizeOutputs(self, y, numClasses):        \n",
    "        y_scaled = []\n",
    "        for y_i in y:\n",
    "            #y_i = mapToUniformlySpacedIntervals(y_i, numClasses)\n",
    "            y_i = mapToNonUniformlySpacedIntervals(y_i)\n",
    "            y_scaled.append(y_i)\n",
    "                \n",
    "        return y_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the global parameters. Set `NUM_CLASSES` to `0` if you want to run the regression flow, otherwise set this to the number of classes (outcome value ranges).\n",
    "\n",
    "The function `convertWordsToIds` converts each word (e.g. `C:5579097:35.7`) into an id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def plotHistogram(valueList, caption):\n",
    "    freqs = pd.Series(valueList).value_counts()\n",
    "    #print (freqs)\n",
    "    freqs.plot(kind='bar')\n",
    "    plt.suptitle(caption)\n",
    "    plt.show()\n",
    "\n",
    "def ascii_histogram(seq, caption) -> None:\n",
    "    print (caption)\n",
    "    counted = Counter(seq)\n",
    "    for k in sorted(counted):\n",
    "        print('{0:5d} {1}'.format(k, '+' * counted[k]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets and embedded vectors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def getSelectedData(x, y, indexes):\n",
    "    x_sel = []\n",
    "    y_sel = []\n",
    "\n",
    "    for index in indexes:\n",
    "        x_sel.append(x[index])\n",
    "        y_sel.append(y[index])\n",
    "\n",
    "    return np.asarray(x_sel), np.asarray(y_sel)\n",
    "    \n",
    "\n",
    "def getTrainTestFromFold(inpH, emb_file, x, y, train_indexes, test_indexes, numClasses):\n",
    "    \n",
    "    #Load the training and the test sets\n",
    "    #Load the text as a sequence of inputs\n",
    "    \n",
    "    x_train, y_train = getSelectedData(x, y, train_indexes)\n",
    "    x_test, y_test = getSelectedData(x, y, test_indexes)\n",
    "    \n",
    "    if numClasses > 0:\n",
    "        plotHistogram(y_train, \"Class labels in training fold\")\n",
    "        plotHistogram(y_test, \"Class labels in test fold\")\n",
    "    \n",
    "    if (numClasses > 0):    \n",
    "        encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "        \n",
    "        #y_all = np.vstack((y_train, y_test))\n",
    "        y_all = np.append(y_train, y_test)\n",
    "\n",
    "        encoder.fit(y_all.reshape(-1, 1))\n",
    "\n",
    "        y_train = encoder.transform(y_train.reshape(-1, 1))\n",
    "        y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "            \n",
    "    #Print the loaded words\n",
    "    nwords=0\n",
    "    for w in inpH.pre_emb:\n",
    "        print (\"Dimension of vectors: {}\".format(inpH.pre_emb[w].shape))\n",
    "        print (\"{} {}\".format(w, inpH.pre_emb[w][0:5]))\n",
    "        nwords = nwords+1\n",
    "        if (nwords >= 2): break\n",
    "\n",
    "    print (\"vocab size: {}\".format(inpH.vocab_size))\n",
    "    print (\"emb-matrix: {}...\".format(inpH.embedding_matrix[1][:5]))\n",
    "    print (inpH.embedding_matrix.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation based training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "OPTIMIZER='rmsprop'\n",
    "ACTIVATION='sigmoid'\n",
    "EPOCHS=30\n",
    "HIDDEN_LAYER_DIM=50\n",
    "DROPOUT=0.1\n",
    "KERNEL_SIZE=5\n",
    "POOL_SIZE=4\n",
    "LSTM_DIM=64 # LSTM Encoding size\n",
    "FILTER_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLSTM_CNN(num_classes, vsize, input_dim, maxlen, emb_matrix):\n",
    "    if (num_classes > 0):\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        eval_metrics = ['accuracy']\n",
    "        activation_fn = 'softmax'\n",
    "        output_dim = num_classes\n",
    "    else:\n",
    "        loss_fn = rmse\n",
    "        eval_metrics = [rmse]\n",
    "        activation_fn = 'linear'\n",
    "        output_dim = 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vsize, \n",
    "                               output_dim=input_dim, \n",
    "                               input_length=maxlen,\n",
    "                               weights=[emb_matrix],\n",
    "                               trainable=False))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Conv1D(FILTER_SIZE,\n",
    "                     KERNEL_SIZE,\n",
    "                     padding='valid',\n",
    "                     activation=ACTIVATION,\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=POOL_SIZE))\n",
    "    model.add(LSTM(LSTM_DIM))\n",
    "    model.add(layers.Dense(output_dim, activation=activation_fn, name='output_vals'))\n",
    "    \n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss = loss_fn,\n",
    "                  metrics=eval_metrics)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(num_classes, vsize, input_dim, maxlen, emb_matrix):\n",
    "    if (num_classes > 0):\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        eval_metrics = ['accuracy']\n",
    "        activation_fn = 'softmax'\n",
    "        output_dim = num_classes\n",
    "    else:\n",
    "        loss_fn = rmse\n",
    "        eval_metrics = [rmse]\n",
    "        activation_fn = 'linear'\n",
    "        output_dim = 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vsize, \n",
    "                               output_dim=input_dim, \n",
    "                               input_length=maxlen,\n",
    "                               weights=[emb_matrix],\n",
    "                               trainable=False))\n",
    "    #model.add(layers.Flatten())\n",
    "    model.add(LSTM(LSTM_DIM))\n",
    "    #model.add(LSTM(32))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    #model.add(layers.Dense(HIDDEN_LAYER_DIM, activation=ACTIVATION))\n",
    "    #model.add(layers.Dense(20, activation=ACTIVATION))\n",
    "    model.add(layers.Dense(output_dim, activation=activation_fn, name='output_vals'))\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss = loss_fn,\n",
    "                  metrics=eval_metrics)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelOnFold(fold_number, model, x_train, y_train, x_test, y_test,\n",
    "                     maxlen, num_classes=0, epochs=EPOCHS):\n",
    "    \n",
    "    x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    BATCH_SIZE = int(len(x_train)/20) # 5% of the training set size\n",
    "    \n",
    "    print (\"Training model...\")\n",
    "    model.fit(x_train, y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        validation_split=0.1,\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=True)\n",
    "    if (num_classes > 0):\n",
    "        print(\"Fold {}: Cross-entropy loss: {:.4f}, Accuracy: {:.4f}\".format(fold_number, loss, accuracy))\n",
    "    else:\n",
    "        print(\"Fold {}: Loss: {:.4f}, RMSE: {:.4f}\".format(fold_number, loss, accuracy))    \n",
    "        \n",
    "    y_preds = model.predict(x_test)\n",
    "    \n",
    "    if num_classes > 0:\n",
    "        plotHistogram(convertSoftmaxToLabels(y_preds), \"Distribution of predicted class labels in {}-th fold\".format(fold_number))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(inpH, x, y, fold_info, emb_file, maxlen, num_classes=0, epochs=EPOCHS):\n",
    "    i=0\n",
    "    avg_metric_value = 0\n",
    "    \n",
    "    #Load the word vectors\n",
    "    print (\"Loading pre-trained vectors...\")\n",
    "    inpH.loadW2V(emb_file)\n",
    "    \n",
    "    print (\"Building model...\")\n",
    "    model = buildModel(num_classes, inpH.vocab_size, inpH.emb_dim, maxlen, inpH.embedding_matrix)\n",
    "    #model = buildLSTM_CNN(num_classes, inpH.vocab_size, inpH.emb_dim, maxlen, inpH.embedding_matrix)\n",
    "    \n",
    "    for train_indexes, test_indexes in fold_info.split(x, y):\n",
    "        x_train, y_train, x_test, y_test = getTrainTestFromFold(\n",
    "                    inpH, emb_file, x, y, train_indexes, test_indexes, num_classes)\n",
    "\n",
    "        avg_metric_value = avg_metric_value + trainModelOnFold(i, model,\n",
    "                                       x_train, y_train, x_test, y_test,\n",
    "                                       maxlen, num_classes, epochs)\n",
    "        i=i+1\n",
    "    return avg_metric_value/float(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSoftmaxToLabels(y_preds):\n",
    "    labels=[]\n",
    "    for i in range(y_preds.shape[0]):\n",
    "        labels.append(np.argmax(y_preds[i]))\n",
    "    print (labels)\n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    NUM_CLASSES = 0 # set this to 0 for regression and a positive value for classification\n",
    "    #NUM_CLASSES = 7 # set this to 0 for regression and a positive value for classification\n",
    "    #DATA_FILE = \"../sentences/all_withwords.tsv\"\n",
    "    #DATA_FILE = \"../sentences/all_wordsonly.tsv\"\n",
    "    DATA_FILE = \"../sentences/all_nodesonly.tsv\"\n",
    "    TO_ADD_VALUE = 0\n",
    "    #For embedding file with concatenated word features use this\n",
    "    #EMB_FILE = \"../graphs/nodevecs/nodes_and_words.vec\"\n",
    "    #EMB_FILE = \"../graphs/nodevecs/words_only.vec\"\n",
    "    #For node vectors only, use this - \n",
    "    EMB_FILE = \"../graphs/nodevecs/refVecs.vec\"\n",
    "    MAXLEN=50\n",
    "    FOLD=5\n",
    "    \n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"ha:d:n:\", [\"appendnumeric=\", \"datafile=\", \"nodevecs=\"])\n",
    "    \n",
    "        for opt, arg in opts:\n",
    "            if opt == '-h':\n",
    "                print ('NodeSequenceRegression.py -i <trainfile> -o <testfile> -a -n <nodevecs>')\n",
    "                sys.exit()\n",
    "            elif opt in (\"-i\", \"--trainfile\"):\n",
    "                DATA_FILE = arg\n",
    "            elif opt in (\"-a\", \"--appendnumeric\"):\n",
    "                TO_ADD_VALUE = 1\n",
    "            elif opt in (\"-n\", \"--nodevecs\"):\n",
    "                EMB_FILE = arg\n",
    "                \n",
    "    except getopt.GetoptError:\n",
    "        print ('usage: NodeSequenceRegression.py -d <datafile> -a -o <resfile> -n <nodevecs>')\n",
    "            \n",
    "    print (\"Training file: %s\" % (DATA_FILE))\n",
    "    print (\"Append numbers: %d\" % (TO_ADD_VALUE))\n",
    "    print (\"Emb file: %s\" % (EMB_FILE))\n",
    "\n",
    "    inpH = InputHelper()\n",
    "    inpH.convertWordsToIds(EMB_FILE)\n",
    "    \n",
    "    VAL_ADDED_EMBFILE = EMB_FILE\n",
    "    embfileDir = os.path.dirname(os.path.realpath(EMB_FILE))\n",
    "\n",
    "    if (TO_ADD_VALUE == 1):\n",
    "        VAL_ADDED_EMBFILE = embfileDir + '/ndvecswithvals.vec' \n",
    "        add_value_feature(EMB_FILE, VAL_ADDED_EMBFILE)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=FOLD, random_state=SEED)    \n",
    "    x, y = inpH.loadDataWithFolds(DATA_FILE, numClasses=NUM_CLASSES)\n",
    "    \n",
    "    avg_metric_value_for_folds = trainModel(inpH, x, y, skf, VAL_ADDED_EMBFILE, maxlen=MAXLEN, num_classes=NUM_CLASSES)\n",
    "    print (\"Avg after {} folds: {}\".format(FOLD, avg_metric_value_for_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: NodeSequenceRegression.py -d <datafile> -a -o <resfile> -n <nodevecs>\n",
      "Training file: ../sentences/all_nodesonly.tsv\n",
      "Append numbers: 0\n",
      "Emb file: ../graphs/nodevecs/refVecs.vec\n",
      "Collecting node names...\n",
      "Collected node names...\n",
      "Converting words to ids...\n",
      "Finished converting words to ids...\n",
      "Loading data from ../sentences/all_nodesonly.tsv\n",
      "Loading pre-trained vectors...\n",
      "Loading W2V data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3492 is corrupt!\n",
      "Line 3507 is corrupt!\n",
      "Line 5805 is corrupt!\n",
      "loaded word2vec for 8182 nodes\n",
      "Building model...\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 50, 128)           1047424   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "output_vals (Dense)          (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,096,897\n",
      "Trainable params: 49,473\n",
      "Non-trainable params: 1,047,424\n",
      "_________________________________________________________________\n",
      "Dimension of vectors: (128,)\n",
      "C:5578602:Lower_caste_37.2 ['0.149615' '0.050254' '-0.006303' '-0.087948' '0.039424']\n",
      "Dimension of vectors: (128,)\n",
      "I:3675717:1 ['0.373619' '0.398837' '-0.081268' '0.143891' '0.308124']\n",
      "vocab size: 8183\n",
      "emb-matrix: [ 0.149615  0.050254 -0.006303 -0.087948  0.039424]...\n",
      "(8183, 128)\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 577 samples, validate on 65 samples\n",
      "Epoch 1/30\n",
      "577/577 [==============================] - 1s 1ms/step - loss: 17.9935 - rmse: 18.0591 - val_loss: 15.9929 - val_rmse: 12.7496\n",
      "Epoch 2/30\n",
      "577/577 [==============================] - 0s 543us/step - loss: 15.0678 - rmse: 14.4526 - val_loss: 14.9326 - val_rmse: 12.7181\n",
      "Epoch 3/30\n",
      "577/577 [==============================] - 0s 619us/step - loss: 14.3093 - rmse: 13.6648 - val_loss: 14.2320 - val_rmse: 12.8080\n",
      "Epoch 4/30\n",
      "577/577 [==============================] - 0s 599us/step - loss: 13.8031 - rmse: 13.3804 - val_loss: 13.9699 - val_rmse: 12.8892\n",
      "Epoch 5/30\n",
      "577/577 [==============================] - 0s 612us/step - loss: 13.4975 - rmse: 13.9263 - val_loss: 13.7116 - val_rmse: 13.0208\n",
      "Epoch 6/30\n",
      "577/577 [==============================] - 0s 569us/step - loss: 13.4957 - rmse: 13.3617 - val_loss: 13.5507 - val_rmse: 13.1525\n",
      "Epoch 7/30\n",
      "577/577 [==============================] - 0s 601us/step - loss: 13.4989 - rmse: 13.1888 - val_loss: 13.5116 - val_rmse: 13.1950\n",
      "Epoch 8/30\n",
      "577/577 [==============================] - 0s 527us/step - loss: 13.5422 - rmse: 13.2776 - val_loss: 13.4728 - val_rmse: 13.2434\n",
      "Epoch 9/30\n",
      "577/577 [==============================] - 0s 556us/step - loss: 13.5267 - rmse: 13.2278 - val_loss: 13.5208 - val_rmse: 13.1571\n",
      "Epoch 10/30\n",
      "577/577 [==============================] - 0s 562us/step - loss: 13.4266 - rmse: 13.2466 - val_loss: 13.7322 - val_rmse: 13.0001\n",
      "Epoch 11/30\n",
      "577/577 [==============================] - 0s 571us/step - loss: 13.4782 - rmse: 13.4624 - val_loss: 13.5827 - val_rmse: 13.0728\n",
      "Epoch 12/30\n",
      "577/577 [==============================] - 0s 592us/step - loss: 13.2688 - rmse: 13.0358 - val_loss: 13.4118 - val_rmse: 13.3660\n",
      "Epoch 13/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 13.2563 - rmse: 12.8109 - val_loss: 13.3129 - val_rmse: 13.5638\n",
      "Epoch 14/30\n",
      "577/577 [==============================] - 0s 538us/step - loss: 13.3627 - rmse: 12.6954 - val_loss: 14.2609 - val_rmse: 12.2642\n",
      "Epoch 15/30\n",
      "577/577 [==============================] - 0s 584us/step - loss: 13.3026 - rmse: 13.1521 - val_loss: 13.3252 - val_rmse: 13.6918\n",
      "Epoch 16/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 13.3086 - rmse: 12.7055 - val_loss: 13.3168 - val_rmse: 13.7553\n",
      "Epoch 17/30\n",
      "577/577 [==============================] - 0s 548us/step - loss: 13.2029 - rmse: 12.8363 - val_loss: 15.4279 - val_rmse: 12.0963\n",
      "Epoch 18/30\n",
      "577/577 [==============================] - 0s 524us/step - loss: 13.1679 - rmse: 12.5176 - val_loss: 13.3965 - val_rmse: 13.4284\n",
      "Epoch 19/30\n",
      "577/577 [==============================] - 0s 525us/step - loss: 13.2439 - rmse: 13.1698 - val_loss: 14.4907 - val_rmse: 12.2076\n",
      "Epoch 20/30\n",
      "577/577 [==============================] - 0s 540us/step - loss: 13.2861 - rmse: 13.6673 - val_loss: 13.7225 - val_rmse: 13.6514\n",
      "Epoch 21/30\n",
      "577/577 [==============================] - 0s 549us/step - loss: 13.2920 - rmse: 12.6862 - val_loss: 14.1237 - val_rmse: 12.3901\n",
      "Epoch 22/30\n",
      "577/577 [==============================] - 0s 539us/step - loss: 12.8461 - rmse: 12.5785 - val_loss: 13.6150 - val_rmse: 13.5013\n",
      "Epoch 23/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 12.9998 - rmse: 12.4251 - val_loss: 13.4776 - val_rmse: 13.3074\n",
      "Epoch 24/30\n",
      "577/577 [==============================] - 0s 562us/step - loss: 12.8392 - rmse: 15.9676 - val_loss: 13.3317 - val_rmse: 13.6411\n",
      "Epoch 25/30\n",
      "577/577 [==============================] - 0s 566us/step - loss: 13.1779 - rmse: 13.0293 - val_loss: 14.5743 - val_rmse: 12.5804\n",
      "Epoch 26/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 13.3598 - rmse: 13.1694 - val_loss: 13.4596 - val_rmse: 13.8517\n",
      "Epoch 27/30\n",
      "577/577 [==============================] - 0s 524us/step - loss: 13.0745 - rmse: 13.4682 - val_loss: 13.5146 - val_rmse: 14.0306\n",
      "Epoch 28/30\n",
      "577/577 [==============================] - 0s 525us/step - loss: 13.1097 - rmse: 12.4755 - val_loss: 13.3882 - val_rmse: 13.6567\n",
      "Epoch 29/30\n",
      "577/577 [==============================] - 0s 536us/step - loss: 13.0186 - rmse: 12.4040 - val_loss: 13.4276 - val_rmse: 13.6296\n",
      "Epoch 30/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 13.0683 - rmse: 12.4260 - val_loss: 14.4665 - val_rmse: 12.1554\n",
      "161/161 [==============================] - 0s 157us/step\n",
      "Fold 0: Loss: 12.1136, RMSE: 10.6970\n",
      "Dimension of vectors: (128,)\n",
      "C:5578602:Lower_caste_37.2 ['0.149615' '0.050254' '-0.006303' '-0.087948' '0.039424']\n",
      "Dimension of vectors: (128,)\n",
      "I:3675717:1 ['0.373619' '0.398837' '-0.081268' '0.143891' '0.308124']\n",
      "vocab size: 8183\n",
      "emb-matrix: [ 0.149615  0.050254 -0.006303 -0.087948  0.039424]...\n",
      "(8183, 128)\n",
      "Training model...\n",
      "Train on 577 samples, validate on 65 samples\n",
      "Epoch 1/30\n",
      "577/577 [==============================] - 0s 526us/step - loss: 12.9358 - rmse: 12.4688 - val_loss: 14.7639 - val_rmse: 12.1120\n",
      "Epoch 2/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 12.6735 - rmse: 12.4667 - val_loss: 13.8334 - val_rmse: 13.5408\n",
      "Epoch 3/30\n",
      "577/577 [==============================] - 0s 525us/step - loss: 12.7722 - rmse: 12.2483 - val_loss: 13.9904 - val_rmse: 12.6885\n",
      "Epoch 4/30\n",
      "577/577 [==============================] - 0s 523us/step - loss: 12.7489 - rmse: 12.5593 - val_loss: 13.7867 - val_rmse: 12.8874\n",
      "Epoch 5/30\n",
      "577/577 [==============================] - 0s 530us/step - loss: 12.8353 - rmse: 12.1885 - val_loss: 14.1687 - val_rmse: 14.9424\n",
      "Epoch 6/30\n",
      "577/577 [==============================] - 0s 530us/step - loss: 12.9486 - rmse: 12.7930 - val_loss: 13.9694 - val_rmse: 12.5943\n",
      "Epoch 7/30\n",
      "577/577 [==============================] - 0s 530us/step - loss: 12.8904 - rmse: 12.8175 - val_loss: 14.1639 - val_rmse: 12.2902\n",
      "Epoch 8/30\n",
      "577/577 [==============================] - 0s 528us/step - loss: 12.7822 - rmse: 12.5325 - val_loss: 14.0242 - val_rmse: 13.0879\n",
      "Epoch 9/30\n",
      "577/577 [==============================] - 0s 536us/step - loss: 12.7803 - rmse: 12.1581 - val_loss: 13.8886 - val_rmse: 13.8102\n",
      "Epoch 10/30\n",
      "577/577 [==============================] - 0s 529us/step - loss: 12.9685 - rmse: 12.3672 - val_loss: 14.0033 - val_rmse: 12.8229\n",
      "Epoch 11/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 12.7849 - rmse: 12.2280 - val_loss: 13.7725 - val_rmse: 13.6213\n",
      "Epoch 12/30\n",
      "577/577 [==============================] - 0s 586us/step - loss: 12.8407 - rmse: 12.7992 - val_loss: 13.6126 - val_rmse: 13.3961\n",
      "Epoch 13/30\n",
      "577/577 [==============================] - 0s 540us/step - loss: 12.8172 - rmse: 13.2766 - val_loss: 13.9247 - val_rmse: 14.1064\n",
      "Epoch 14/30\n",
      "577/577 [==============================] - 0s 582us/step - loss: 12.8464 - rmse: 12.2497 - val_loss: 13.9165 - val_rmse: 14.2671\n",
      "Epoch 15/30\n",
      "577/577 [==============================] - 0s 540us/step - loss: 12.9503 - rmse: 12.6739 - val_loss: 13.8878 - val_rmse: 14.2632\n",
      "Epoch 16/30\n",
      "577/577 [==============================] - 0s 545us/step - loss: 12.8401 - rmse: 12.3879 - val_loss: 13.8726 - val_rmse: 13.0371\n",
      "Epoch 17/30\n",
      "577/577 [==============================] - 0s 566us/step - loss: 12.5185 - rmse: 15.8564 - val_loss: 13.6150 - val_rmse: 13.3397\n",
      "Epoch 18/30\n",
      "577/577 [==============================] - 0s 536us/step - loss: 12.7558 - rmse: 12.6504 - val_loss: 13.7459 - val_rmse: 13.5347\n",
      "Epoch 19/30\n",
      "577/577 [==============================] - 0s 561us/step - loss: 12.7785 - rmse: 12.4746 - val_loss: 13.6559 - val_rmse: 13.2337\n",
      "Epoch 20/30\n",
      "577/577 [==============================] - 0s 544us/step - loss: 12.7856 - rmse: 13.0756 - val_loss: 13.7505 - val_rmse: 13.4669\n",
      "Epoch 21/30\n",
      "577/577 [==============================] - 0s 538us/step - loss: 12.6501 - rmse: 13.3147 - val_loss: 13.5682 - val_rmse: 13.5724\n",
      "Epoch 22/30\n",
      "577/577 [==============================] - 0s 558us/step - loss: 12.7409 - rmse: 12.4043 - val_loss: 13.5702 - val_rmse: 13.5410\n",
      "Epoch 23/30\n",
      "577/577 [==============================] - 0s 534us/step - loss: 12.8667 - rmse: 12.5197 - val_loss: 13.8567 - val_rmse: 13.9821\n",
      "Epoch 24/30\n",
      "577/577 [==============================] - 0s 539us/step - loss: 12.8350 - rmse: 12.3472 - val_loss: 13.9122 - val_rmse: 13.0625\n",
      "Epoch 25/30\n",
      "577/577 [==============================] - 0s 547us/step - loss: 13.0269 - rmse: 12.4621 - val_loss: 13.8076 - val_rmse: 13.9708\n",
      "Epoch 26/30\n",
      "577/577 [==============================] - 0s 545us/step - loss: 12.8482 - rmse: 12.4824 - val_loss: 13.8244 - val_rmse: 12.9521\n",
      "Epoch 27/30\n",
      "577/577 [==============================] - 0s 554us/step - loss: 12.7180 - rmse: 12.8153 - val_loss: 14.0169 - val_rmse: 13.7208\n",
      "Epoch 28/30\n",
      "577/577 [==============================] - 0s 563us/step - loss: 12.7990 - rmse: 12.2010 - val_loss: 14.8259 - val_rmse: 11.6341\n",
      "Epoch 29/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 12.7377 - rmse: 13.3400 - val_loss: 14.1595 - val_rmse: 14.2184\n",
      "Epoch 30/30\n",
      "577/577 [==============================] - 0s 609us/step - loss: 12.8749 - rmse: 12.2961 - val_loss: 13.6778 - val_rmse: 13.3106\n",
      "161/161 [==============================] - 0s 153us/step\n",
      "Fold 1: Loss: 12.4470, RMSE: 10.5889\n",
      "Dimension of vectors: (128,)\n",
      "C:5578602:Lower_caste_37.2 ['0.149615' '0.050254' '-0.006303' '-0.087948' '0.039424']\n",
      "Dimension of vectors: (128,)\n",
      "I:3675717:1 ['0.373619' '0.398837' '-0.081268' '0.143891' '0.308124']\n",
      "vocab size: 8183\n",
      "emb-matrix: [ 0.149615  0.050254 -0.006303 -0.087948  0.039424]...\n",
      "(8183, 128)\n",
      "Training model...\n",
      "Train on 577 samples, validate on 65 samples\n",
      "Epoch 1/30\n",
      "577/577 [==============================] - 0s 547us/step - loss: 12.6930 - rmse: 12.3457 - val_loss: 12.1686 - val_rmse: 11.4504\n",
      "Epoch 2/30\n",
      "577/577 [==============================] - 0s 554us/step - loss: 12.5715 - rmse: 12.8544 - val_loss: 13.1218 - val_rmse: 13.3304\n",
      "Epoch 3/30\n",
      "577/577 [==============================] - 0s 555us/step - loss: 12.5380 - rmse: 12.4331 - val_loss: 12.7489 - val_rmse: 12.7435\n",
      "Epoch 4/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 12.5671 - rmse: 11.9784 - val_loss: 13.7846 - val_rmse: 14.7715\n",
      "Epoch 5/30\n",
      "577/577 [==============================] - 0s 556us/step - loss: 12.7645 - rmse: 12.4314 - val_loss: 12.9401 - val_rmse: 13.3544\n",
      "Epoch 6/30\n",
      "577/577 [==============================] - 0s 548us/step - loss: 12.5676 - rmse: 12.1346 - val_loss: 12.5805 - val_rmse: 12.0797\n",
      "Epoch 7/30\n",
      "577/577 [==============================] - 0s 534us/step - loss: 12.7079 - rmse: 12.3454 - val_loss: 12.5216 - val_rmse: 11.4788\n",
      "Epoch 8/30\n",
      "577/577 [==============================] - 0s 543us/step - loss: 12.6583 - rmse: 12.2265 - val_loss: 12.8235 - val_rmse: 13.2016\n",
      "Epoch 9/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 12.3868 - rmse: 12.4866 - val_loss: 13.1851 - val_rmse: 13.5013\n",
      "Epoch 10/30\n",
      "577/577 [==============================] - 0s 532us/step - loss: 12.6008 - rmse: 12.1010 - val_loss: 12.4955 - val_rmse: 12.4851\n",
      "Epoch 11/30\n",
      "577/577 [==============================] - 0s 539us/step - loss: 12.5143 - rmse: 12.4883 - val_loss: 12.3975 - val_rmse: 10.9880\n",
      "Epoch 12/30\n",
      "577/577 [==============================] - 0s 562us/step - loss: 12.5603 - rmse: 12.5760 - val_loss: 12.2834 - val_rmse: 11.9892\n",
      "Epoch 13/30\n",
      "577/577 [==============================] - 0s 549us/step - loss: 12.5688 - rmse: 12.2862 - val_loss: 12.4320 - val_rmse: 12.1729\n",
      "Epoch 14/30\n",
      "577/577 [==============================] - 0s 523us/step - loss: 12.5233 - rmse: 12.5005 - val_loss: 12.3075 - val_rmse: 12.3836\n",
      "Epoch 15/30\n",
      "577/577 [==============================] - 0s 522us/step - loss: 12.5758 - rmse: 12.1454 - val_loss: 13.2840 - val_rmse: 13.5848\n",
      "Epoch 16/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 12.5613 - rmse: 12.3444 - val_loss: 12.3142 - val_rmse: 11.7601\n",
      "Epoch 17/30\n",
      "577/577 [==============================] - 0s 534us/step - loss: 12.5922 - rmse: 12.2015 - val_loss: 12.7087 - val_rmse: 12.8421\n",
      "Epoch 18/30\n",
      "577/577 [==============================] - 0s 537us/step - loss: 12.4229 - rmse: 12.1376 - val_loss: 12.5829 - val_rmse: 11.0034\n",
      "Epoch 19/30\n",
      "577/577 [==============================] - 0s 533us/step - loss: 12.5384 - rmse: 12.3963 - val_loss: 12.4176 - val_rmse: 11.1516\n",
      "Epoch 20/30\n",
      "577/577 [==============================] - 0s 540us/step - loss: 12.7074 - rmse: 12.6537 - val_loss: 12.7667 - val_rmse: 12.9451\n",
      "Epoch 21/30\n",
      "577/577 [==============================] - 0s 537us/step - loss: 12.5716 - rmse: 12.3898 - val_loss: 12.2714 - val_rmse: 11.3963\n",
      "Epoch 22/30\n",
      "577/577 [==============================] - 0s 541us/step - loss: 12.5963 - rmse: 12.8978 - val_loss: 13.1168 - val_rmse: 13.3758\n",
      "Epoch 23/30\n",
      "577/577 [==============================] - 0s 541us/step - loss: 12.6188 - rmse: 12.0718 - val_loss: 12.5270 - val_rmse: 12.4346\n",
      "Epoch 24/30\n",
      "577/577 [==============================] - 0s 534us/step - loss: 12.5998 - rmse: 12.2810 - val_loss: 12.3819 - val_rmse: 11.6973\n",
      "Epoch 25/30\n",
      "577/577 [==============================] - 0s 522us/step - loss: 12.4594 - rmse: 12.3580 - val_loss: 12.4331 - val_rmse: 11.0324\n",
      "Epoch 26/30\n",
      "577/577 [==============================] - 0s 540us/step - loss: 12.5740 - rmse: 12.0569 - val_loss: 12.2636 - val_rmse: 11.3682\n",
      "Epoch 27/30\n",
      "577/577 [==============================] - 0s 551us/step - loss: 12.4346 - rmse: 12.1264 - val_loss: 12.5923 - val_rmse: 10.8661\n",
      "Epoch 28/30\n",
      "577/577 [==============================] - 0s 541us/step - loss: 12.4088 - rmse: 11.7767 - val_loss: 12.5909 - val_rmse: 12.7750\n",
      "Epoch 29/30\n",
      "577/577 [==============================] - 0s 535us/step - loss: 12.5850 - rmse: 12.0445 - val_loss: 12.5068 - val_rmse: 10.8799\n",
      "Epoch 30/30\n",
      "577/577 [==============================] - 0s 529us/step - loss: 12.4599 - rmse: 12.1953 - val_loss: 12.5826 - val_rmse: 12.9008\n",
      "161/161 [==============================] - 0s 144us/step\n",
      "Fold 2: Loss: 13.6339, RMSE: 13.1738\n",
      "Dimension of vectors: (128,)\n",
      "C:5578602:Lower_caste_37.2 ['0.149615' '0.050254' '-0.006303' '-0.087948' '0.039424']\n",
      "Dimension of vectors: (128,)\n",
      "I:3675717:1 ['0.373619' '0.398837' '-0.081268' '0.143891' '0.308124']\n",
      "vocab size: 8183\n",
      "emb-matrix: [ 0.149615  0.050254 -0.006303 -0.087948  0.039424]...\n",
      "(8183, 128)\n",
      "Training model...\n",
      "Train on 578 samples, validate on 65 samples\n",
      "Epoch 1/30\n",
      "578/578 [==============================] - 0s 536us/step - loss: 12.1915 - rmse: 11.8989 - val_loss: 15.1582 - val_rmse: 12.7468\n",
      "Epoch 2/30\n",
      "578/578 [==============================] - 0s 546us/step - loss: 12.2933 - rmse: 12.0437 - val_loss: 14.6822 - val_rmse: 13.4232\n",
      "Epoch 3/30\n",
      "578/578 [==============================] - 0s 541us/step - loss: 12.3588 - rmse: 12.0065 - val_loss: 14.8784 - val_rmse: 13.0024\n",
      "Epoch 4/30\n",
      "578/578 [==============================] - 0s 537us/step - loss: 12.2476 - rmse: 11.9229 - val_loss: 14.9497 - val_rmse: 13.5419\n",
      "Epoch 5/30\n",
      "578/578 [==============================] - 0s 541us/step - loss: 12.2686 - rmse: 12.0145 - val_loss: 15.0545 - val_rmse: 12.9987\n",
      "Epoch 6/30\n",
      "578/578 [==============================] - 0s 560us/step - loss: 12.1662 - rmse: 12.5982 - val_loss: 14.5575 - val_rmse: 14.4480\n",
      "Epoch 7/30\n",
      "578/578 [==============================] - 0s 553us/step - loss: 12.3068 - rmse: 11.8130 - val_loss: 14.8523 - val_rmse: 12.8467\n",
      "Epoch 8/30\n",
      "578/578 [==============================] - 0s 540us/step - loss: 12.1698 - rmse: 11.8942 - val_loss: 14.9753 - val_rmse: 12.8473\n",
      "Epoch 9/30\n",
      "578/578 [==============================] - 0s 542us/step - loss: 12.3515 - rmse: 11.9887 - val_loss: 14.7436 - val_rmse: 13.8383\n",
      "Epoch 10/30\n",
      "578/578 [==============================] - 0s 548us/step - loss: 12.3341 - rmse: 11.9643 - val_loss: 14.6684 - val_rmse: 13.5607\n",
      "Epoch 11/30\n",
      "578/578 [==============================] - 0s 541us/step - loss: 12.3632 - rmse: 12.3198 - val_loss: 14.7850 - val_rmse: 13.2971\n",
      "Epoch 12/30\n",
      "578/578 [==============================] - 0s 536us/step - loss: 12.4132 - rmse: 11.9930 - val_loss: 14.7860 - val_rmse: 13.0714\n",
      "Epoch 13/30\n",
      "578/578 [==============================] - 0s 539us/step - loss: 12.2089 - rmse: 11.8930 - val_loss: 14.8713 - val_rmse: 12.7617\n",
      "Epoch 14/30\n",
      "578/578 [==============================] - 0s 535us/step - loss: 12.2104 - rmse: 11.9889 - val_loss: 15.1231 - val_rmse: 12.8642\n",
      "Epoch 15/30\n",
      "578/578 [==============================] - 0s 540us/step - loss: 11.9691 - rmse: 11.5743 - val_loss: 15.0099 - val_rmse: 13.0464\n",
      "Epoch 16/30\n",
      "578/578 [==============================] - 0s 563us/step - loss: 12.2088 - rmse: 11.7785 - val_loss: 15.1624 - val_rmse: 12.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "578/578 [==============================] - 0s 539us/step - loss: 12.1993 - rmse: 12.1215 - val_loss: 14.5931 - val_rmse: 13.5705\n",
      "Epoch 18/30\n",
      "578/578 [==============================] - 0s 545us/step - loss: 12.2634 - rmse: 12.0797 - val_loss: 14.5163 - val_rmse: 13.6029\n",
      "Epoch 19/30\n",
      "578/578 [==============================] - 0s 556us/step - loss: 12.1325 - rmse: 11.9225 - val_loss: 14.9894 - val_rmse: 12.7986\n",
      "Epoch 20/30\n",
      "578/578 [==============================] - 0s 543us/step - loss: 12.2341 - rmse: 12.0507 - val_loss: 15.0445 - val_rmse: 12.9531\n",
      "Epoch 21/30\n",
      "578/578 [==============================] - 0s 537us/step - loss: 12.1393 - rmse: 12.1314 - val_loss: 14.3992 - val_rmse: 13.9336\n",
      "Epoch 22/30\n",
      "578/578 [==============================] - 0s 552us/step - loss: 12.1023 - rmse: 11.9248 - val_loss: 14.9577 - val_rmse: 12.8209\n",
      "Epoch 23/30\n",
      "578/578 [==============================] - 0s 560us/step - loss: 11.8633 - rmse: 12.6379 - val_loss: 14.2802 - val_rmse: 14.1927\n",
      "Epoch 24/30\n",
      "578/578 [==============================] - 0s 553us/step - loss: 11.9034 - rmse: 11.6974 - val_loss: 14.9477 - val_rmse: 12.9951\n",
      "Epoch 25/30\n",
      "578/578 [==============================] - 0s 572us/step - loss: 11.9640 - rmse: 11.4961 - val_loss: 15.2589 - val_rmse: 12.9468\n",
      "Epoch 26/30\n",
      "578/578 [==============================] - 0s 529us/step - loss: 12.2325 - rmse: 12.0975 - val_loss: 15.0215 - val_rmse: 13.0192\n",
      "Epoch 27/30\n",
      "578/578 [==============================] - 0s 583us/step - loss: 12.1056 - rmse: 12.0637 - val_loss: 14.9671 - val_rmse: 13.6017\n",
      "Epoch 28/30\n",
      "578/578 [==============================] - 0s 545us/step - loss: 12.0843 - rmse: 11.8615 - val_loss: 15.2272 - val_rmse: 12.8913\n",
      "Epoch 29/30\n",
      "578/578 [==============================] - 0s 534us/step - loss: 12.0251 - rmse: 11.7640 - val_loss: 15.2336 - val_rmse: 12.7711\n",
      "Epoch 30/30\n",
      "578/578 [==============================] - 0s 570us/step - loss: 12.0175 - rmse: 11.7424 - val_loss: 14.4489 - val_rmse: 14.3448\n",
      "160/160 [==============================] - 0s 142us/step\n",
      "Fold 3: Loss: 13.5066, RMSE: 13.5066\n",
      "Dimension of vectors: (128,)\n",
      "C:5578602:Lower_caste_37.2 ['0.149615' '0.050254' '-0.006303' '-0.087948' '0.039424']\n",
      "Dimension of vectors: (128,)\n",
      "I:3675717:1 ['0.373619' '0.398837' '-0.081268' '0.143891' '0.308124']\n",
      "vocab size: 8183\n",
      "emb-matrix: [ 0.149615  0.050254 -0.006303 -0.087948  0.039424]...\n",
      "(8183, 128)\n",
      "Training model...\n",
      "Train on 578 samples, validate on 65 samples\n",
      "Epoch 1/30\n",
      "578/578 [==============================] - 0s 541us/step - loss: 12.6711 - rmse: 12.8700 - val_loss: 12.8061 - val_rmse: 12.9615\n",
      "Epoch 2/30\n",
      "578/578 [==============================] - 0s 538us/step - loss: 12.6566 - rmse: 12.3467 - val_loss: 12.8689 - val_rmse: 13.4623\n",
      "Epoch 3/30\n",
      "578/578 [==============================] - 0s 586us/step - loss: 12.7015 - rmse: 12.4918 - val_loss: 12.8256 - val_rmse: 13.0669\n",
      "Epoch 4/30\n",
      "578/578 [==============================] - 0s 544us/step - loss: 12.5347 - rmse: 12.5242 - val_loss: 12.8670 - val_rmse: 12.0305\n",
      "Epoch 5/30\n",
      "578/578 [==============================] - 0s 537us/step - loss: 12.5908 - rmse: 13.6800 - val_loss: 12.8687 - val_rmse: 12.3008\n",
      "Epoch 6/30\n",
      "578/578 [==============================] - 0s 544us/step - loss: 12.6024 - rmse: 12.0199 - val_loss: 12.8050 - val_rmse: 12.7257\n",
      "Epoch 7/30\n",
      "578/578 [==============================] - 0s 565us/step - loss: 12.6266 - rmse: 12.2368 - val_loss: 13.2738 - val_rmse: 12.0547\n",
      "Epoch 8/30\n",
      "578/578 [==============================] - 0s 542us/step - loss: 12.4611 - rmse: 12.4802 - val_loss: 13.3292 - val_rmse: 12.3179\n",
      "Epoch 9/30\n",
      "578/578 [==============================] - 0s 528us/step - loss: 12.5639 - rmse: 12.1668 - val_loss: 13.6097 - val_rmse: 13.0389\n",
      "Epoch 10/30\n",
      "578/578 [==============================] - 0s 534us/step - loss: 12.4821 - rmse: 12.2578 - val_loss: 12.9519 - val_rmse: 13.1058\n",
      "Epoch 11/30\n",
      "578/578 [==============================] - 0s 545us/step - loss: 12.5868 - rmse: 12.1272 - val_loss: 12.6829 - val_rmse: 13.1339\n",
      "Epoch 12/30\n",
      "578/578 [==============================] - 0s 525us/step - loss: 12.6698 - rmse: 12.5099 - val_loss: 12.8640 - val_rmse: 13.0058\n",
      "Epoch 13/30\n",
      "578/578 [==============================] - 0s 531us/step - loss: 12.6802 - rmse: 13.9458 - val_loss: 13.0317 - val_rmse: 12.8158\n",
      "Epoch 14/30\n",
      "578/578 [==============================] - 0s 536us/step - loss: 12.4533 - rmse: 12.5599 - val_loss: 12.9435 - val_rmse: 12.1780\n",
      "Epoch 15/30\n",
      "578/578 [==============================] - 0s 537us/step - loss: 12.3863 - rmse: 11.9870 - val_loss: 12.5538 - val_rmse: 12.8569\n",
      "Epoch 16/30\n",
      "578/578 [==============================] - 0s 526us/step - loss: 12.5357 - rmse: 12.4450 - val_loss: 12.5788 - val_rmse: 12.6132\n",
      "Epoch 17/30\n",
      "578/578 [==============================] - 0s 536us/step - loss: 12.6661 - rmse: 12.2936 - val_loss: 13.0736 - val_rmse: 13.4683\n",
      "Epoch 18/30\n",
      "578/578 [==============================] - ETA: 0s - loss: 12.8242 - rmse: 12.82 - 0s 572us/step - loss: 12.5441 - rmse: 12.5225 - val_loss: 12.5555 - val_rmse: 12.0379\n",
      "Epoch 19/30\n",
      "578/578 [==============================] - 0s 599us/step - loss: 12.3752 - rmse: 12.0438 - val_loss: 12.7256 - val_rmse: 12.7031\n",
      "Epoch 20/30\n",
      "578/578 [==============================] - 0s 569us/step - loss: 12.3103 - rmse: 12.7647 - val_loss: 13.5117 - val_rmse: 13.0645\n",
      "Epoch 21/30\n",
      "578/578 [==============================] - 0s 564us/step - loss: 12.4095 - rmse: 12.0581 - val_loss: 12.9340 - val_rmse: 12.0572\n",
      "Epoch 22/30\n",
      "578/578 [==============================] - 0s 553us/step - loss: 12.1495 - rmse: 11.8108 - val_loss: 13.2742 - val_rmse: 12.2051\n",
      "Epoch 23/30\n",
      "578/578 [==============================] - 0s 560us/step - loss: 12.4574 - rmse: 12.4271 - val_loss: 13.6617 - val_rmse: 12.5339\n",
      "Epoch 24/30\n",
      "578/578 [==============================] - 0s 540us/step - loss: 12.6347 - rmse: 12.4712 - val_loss: 12.7076 - val_rmse: 12.8832\n",
      "Epoch 25/30\n",
      "578/578 [==============================] - 0s 535us/step - loss: 12.8395 - rmse: 12.9278 - val_loss: 12.7839 - val_rmse: 12.0837\n",
      "Epoch 26/30\n",
      "578/578 [==============================] - 0s 543us/step - loss: 12.6946 - rmse: 12.3396 - val_loss: 13.0651 - val_rmse: 12.4492\n",
      "Epoch 27/30\n",
      "578/578 [==============================] - 0s 537us/step - loss: 12.2788 - rmse: 12.0149 - val_loss: 12.5549 - val_rmse: 12.7953\n",
      "Epoch 28/30\n",
      "578/578 [==============================] - 0s 522us/step - loss: 12.2034 - rmse: 11.9192 - val_loss: 13.1691 - val_rmse: 13.3432\n",
      "Epoch 29/30\n",
      "578/578 [==============================] - 0s 536us/step - loss: 12.5876 - rmse: 12.4986 - val_loss: 13.2282 - val_rmse: 13.0749\n",
      "Epoch 30/30\n",
      "578/578 [==============================] - 0s 544us/step - loss: 12.3791 - rmse: 11.8924 - val_loss: 12.9220 - val_rmse: 12.6340\n",
      "160/160 [==============================] - 0s 136us/step\n",
      "Fold 4: Loss: 11.7980, RMSE: 11.7980\n",
      "Avg after 5 folds: 11.952856063842773\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "    #main('-i ../sentences/train.tsv -e ../sentences/test.tsv -a -o predictions.txt -n ../graphs/nodevecs/refVecs.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
