{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict\n",
    "import random as r\n",
    "import math as m\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from random import Random\n",
    "\n",
    "BASEDIR = '../../core/prediction/'\n",
    "TRAIN_FILE = BASEDIR + 'sentences/train.tsv'\n",
    "PT_VEC_FILE = BASEDIR + 'graphs/nodevecs/nodes_and_words.vec'\n",
    "\n",
    "MAXLEN = 50\n",
    "SEED=314159 # first digits of Pi... an elegant seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCTArm:\n",
    "    def __init__(self, line, index):\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        self.id = index\n",
    "        self.text = l[0]\n",
    "        self.ov = float(l[1])\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'RCT:({}) OV: {}'.format(self.text, self.ov)\n",
    "    \n",
    "class RCTArms:\n",
    "    \n",
    "    def __init__(self, train_file):\n",
    "        self.rcts = []\n",
    "        \n",
    "        line_number = 0\n",
    "        for line in open(train_file):\n",
    "            rct_arm = RCTArm(line, line_number)\n",
    "            self.rcts.append(rct_arm)\n",
    "            line_number += 1 \n",
    "                \n",
    "    def convertWordsToIds(self, maxlen=MAXLEN):        \n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        all_text = []\n",
    "        for rct in self.rcts:\n",
    "            all_text.append(rct.text)\n",
    "            \n",
    "        self.keras_tokenizer = Tokenizer(num_words=None, filters=[], lower=False, split=' ')\n",
    "        self.keras_tokenizer.fit_on_texts(all_text)\n",
    "        \n",
    "        self.vsize = len(self.keras_tokenizer.word_index) + 1\n",
    "        self.x = self.keras_tokenizer.texts_to_sequences(all_text)\n",
    "        self.x = pad_sequences(self.x, padding='post', maxlen=maxlen)        \n",
    "    \n",
    "    def create_embedding_matrix(self, embfile):\n",
    "        in_dict = 0\n",
    "        \n",
    "        with open(embfile) as f:\n",
    "            line_number=0\n",
    "            \n",
    "            for line in f:\n",
    "                if line_number==0:\n",
    "                    tokens = line.strip().split(\" \")\n",
    "                    self.embedding_dim = int(tokens[1])\n",
    "                    self.embedding_matrix = np.zeros((self.vsize, self.embedding_dim))\n",
    "                else:\n",
    "                    word, *vector = line.split()\n",
    "                    if word in self.keras_tokenizer.word_index:\n",
    "                        idx = self.keras_tokenizer.word_index[word]\n",
    "                        in_dict += 1\n",
    "                        self.embedding_matrix[idx] = np.array(\n",
    "                            vector, dtype=np.float32)[:self.embedding_dim]\n",
    "                \n",
    "                line_number += 1\n",
    "        \n",
    "        return in_dict\n",
    "    \n",
    "    def getData(self, index):\n",
    "        return self.x[index]\n",
    "    \n",
    "    def create_pairs(self):\n",
    "        '''Positive and negative pair creation.\n",
    "        Alternates between positive and negative pairs.\n",
    "        '''\n",
    "        pairs = []\n",
    "        labels = []\n",
    "        \n",
    "        N = len(self.rcts)\n",
    "        for i in range (N-1):\n",
    "            for j in range(i+1,N):                \n",
    "                pairs.append([self.x[i], self.x[j]])\n",
    "                label = 0                \n",
    "                if self.rcts[i].ov >= self.rcts[j].ov:\n",
    "                    label = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        return np.array(pairs), np.array(labels)    \n",
    "    \n",
    "    # Cartesian product between two sets\n",
    "    def create_pairs_from_ids(self, listA, listB):\n",
    "        pairs = []\n",
    "        labels = []\n",
    "        seen_pairs = {}\n",
    "        \n",
    "        self_pair_count = 0\n",
    "        seen_pair_count = 0\n",
    "        \n",
    "        for a in listA:\n",
    "            for b in listB:\n",
    "                '''\n",
    "                Avoid self-pairs; also ignore the order\n",
    "                '''\n",
    "                if a==b:\n",
    "                    self_pair_count+=1\n",
    "                    continue\n",
    "                key = str(a) + ' ' + str(b)\n",
    "                revkey = str(b) + ' ' + str(a)\n",
    "                if not key in seen_pairs and not revkey in seen_pairs:\n",
    "                    seen_pairs[key] = True\n",
    "                    seen_pairs[revkey] = True\n",
    "                else:\n",
    "                    seen_pair_count+=1\n",
    "                    continue # this pair has already been added\n",
    "                    \n",
    "                pairs.append([self.x[a], self.x[b]])\n",
    "                label = 0                \n",
    "                if self.rcts[a].ov >= self.rcts[b].ov:\n",
    "                    label = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print ('Ignored {} (self) and {} (seen) duplicate pairs'.format(self_pair_count, seen_pair_count))\n",
    "        return pairs, labels, seen_pairs\n",
    "        \n",
    "    def create_split_aware_pairs(self, train_ratio=0.9):\n",
    "        #First split the rcts into train and test\n",
    "        r = Random(SEED)\n",
    "        r.shuffle(self.rcts)\n",
    "        \n",
    "        ids = list(map(lambda r: r.id, self.rcts)) # list of shuffled ids\n",
    "        ntrain = int(len(ids)*train_ratio)\n",
    "        train_ids = ids[0:ntrain]\n",
    "        test_ids = ids[ntrain:]\n",
    "        \n",
    "        #collect all pairs from the train ids\n",
    "        train_pairs, train_labels = self.create_pairs_from_ids(train_ids, train_ids)\n",
    "        \n",
    "        #collect all pairs from the test ids - complete subgraph\n",
    "        self_test_pairs, self_test_labels = self.create_pairs_from_ids(test_ids, test_ids)\n",
    "        \n",
    "        # additionally build the cross-pairs, (test, train) pairs\n",
    "        cross_test_pairs, cross_test_labels = self.create_pairs_from_ids(test_ids, train_ids)\n",
    "        \n",
    "        test_pairs = self_test_pairs + cross_test_pairs \n",
    "        test_labels = self_test_labels + cross_test_labels \n",
    "        \n",
    "        return np.array(train_pairs), np.array(train_labels), np.array(test_pairs), np.array(test_labels)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcts = RCTArms(TRAIN_FILE)\n",
    "rcts.convertWordsToIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9839695180557463\n",
      "29131\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings from dictionary \n",
    "nwords_in_dict = rcts.create_embedding_matrix(PT_VEC_FILE)\n",
    "\n",
    "# Print Vocab overlap\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(rcts.embedding_matrix, axis=1))\n",
    "print(nonzero_elements / rcts.vsize)\n",
    "print (nwords_in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored 957 (self) and 457446 (seen) duplicate pairs\n",
      "Ignored 107 (self) and 5671 (seen) duplicate pairs\n",
      "Ignored 0 (self) and 0 (seen) duplicate pairs\n",
      "#Train pairs: 457446\n",
      "#Test pairs: 108070\n",
      "Train Class distribution: \n",
      "1    230335\n",
      "0    227111\n",
      "dtype: int64\n",
      "Test Class distribution: \n",
      "1    55482\n",
      "0    52588\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_train, y_train, x_test, y_test = rcts.create_split_aware_pairs()\n",
    "print (\"#Train pairs: {}\".format(x_train.shape[0]))\n",
    "print (\"#Test pairs: {}\".format(x_test.shape[0]))\n",
    "\n",
    "freqs = pd.Series(y_train).value_counts()\n",
    "print ('Train Class distribution: ')\n",
    "print (freqs)\n",
    "freqs = pd.Series(y_test).value_counts()\n",
    "print ('Test Class distribution: ')\n",
    "print (freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, LSTM, Bidirectional, Concatenate\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (823402, 2, 50), (823402,)\n",
      "Valid set: (91490, 2, 50), (91490,)\n",
      "Test set: (113741, 2, 50), (113741,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Training set: {}, {}\".format(x_train.shape, y_train.shape))\n",
    "print (\"Valid set: {}, {}\".format(x_val.shape, y_val.shape))\n",
    "print (\"Test set: {}, {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DIM=32\n",
    "DROPOUT=0.2\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def complete_model(rcts):\n",
    "    \n",
    "    input_a = Input(shape=(rcts.maxlen, ))    \n",
    "    print (input_a.shape)\n",
    "    \n",
    "    emb_a = Embedding(rcts.embedding_matrix.shape[0],\n",
    "                  rcts.embedding_matrix.shape[1],\n",
    "                  weights=[rcts.embedding_matrix])(input_a)\n",
    "    print (emb_a.shape)\n",
    "    \n",
    "    input_b = Input(shape=(rcts.maxlen, ))    \n",
    "    print (input_b.shape)\n",
    "    \n",
    "    emb_b = Embedding(input_dim=rcts.embedding_matrix.shape[0],\n",
    "                  output_dim=rcts.embedding_matrix.shape[1],\n",
    "                  weights=[rcts.embedding_matrix])(input_b)\n",
    "    print (emb_b.shape)\n",
    "    \n",
    "    shared_lstm = LSTM(LSTM_DIM)\n",
    "\n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = shared_lstm(emb_a)\n",
    "    processed_a = Dropout(DROPOUT)(processed_a)\n",
    "    processed_b = shared_lstm(emb_b)\n",
    "    processed_b = Dropout(DROPOUT)(processed_b)\n",
    "\n",
    "    merged_vector = concatenate([processed_a, processed_b], axis=-1)\n",
    "    # And add a logistic regression (2 class - sigmoid) on top\n",
    "    # used for backpropagating from the (pred, true) labels\n",
    "    predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "    \n",
    "    model = Model([input_a, input_b], outputs=predictions)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50)\n",
      "(None, 50, 305)\n",
      "(None, 50)\n",
      "(None, 50, 305)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 305)      8885260     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 305)      8885260     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           43264       embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 17,813,849\n",
      "Trainable params: 17,813,849\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = complete_model(rcts)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 823402 samples, validate on 91490 samples\n",
      "Epoch 1/2\n",
      "414000/823402 [==============>...............] - ETA: 4:14 - loss: 0.1998 - accuracy: 0.9143"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91bbad579543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m          )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/hbcp/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS=2\n",
    "BATCH_SIZE=1000\n",
    "model.fit([x_train[:, 0], x_train[:, 1]], y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=([x_val[:, 0], x_val[:, 1]], y_val),\n",
    "          verbose=True\n",
    "         )\n",
    "\n",
    "model.save_weights(\"pairwise-ov-comp-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final accuracy on test set\n",
    "loss, acc = model.evaluate([x_test[:, 0], x_test[:, 1]], y_test, verbose=True)\n",
    "print ('Test Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
