# on OSX you can access the various services on the host by modifying the address with host.docker.internal
# to run the docker compose, run the following command:
# docker-compose up --build

version: "3.8"

services:

  hbcp-core:
    container_name: hbcp-core
    build:
      context: ./core
      #args:
        #- ENTITIES_FOR_PREDICTION=src/main/resources/data/jsons/All_annotations_512papers_05March20_reduced.json
    image: hbcp-core
    ports:
      - "8080:8080"
    environment:
      FLAIR_URL: hbcp-flair-tagger
      FLAIR_PORT: 5000
      PREDICTION_URL: hbcp-prediction-experiments
      PREDICTION_PORT: 5001
      PREDICTION_API_ONLY: "true"

  hbcp-flair-tagger:
    container_name: hbcp-flair-tagger
    build: ./flair-tagger/flair
    image: hbcp-flair-tagger
    environment:
      PORT: 5000
    ports:
      - "5000:5000"      

  hbcp-prediction-experiments:
    container_name: hbcp-prediction-experiments
    build: ./prediction-experiments
    image: hbcp-prediction-experiments
    depends_on:
      - hbcp-core
    volumes:
      - prediction_tf_serving_models:/python-nb/ov-predict/tf_models/
    environment:
      PORT: 5001
      TF_SERVING_HOSTNAME: hbcp-prediction-tf-serving
      TF_SERVING_PORT: 8501
      LOGGING_LEVEL: INFO
    ports:
      - "5001:5001"
      
  hbcp-prediction-tf-serving:
    container_name: hbcp-prediction-tf-serving
    build: ./prediction-experiments/python-nb/ov-predict
    image: hbcp-prediction-tf-serving
    depends_on:
      - hbcp-prediction-experiments
    volumes:
      - prediction_tf_serving_models:/models/
    ports:
      - "8501:8501"

volumes:
  prediction_tf_serving_models:
    driver_opts:
      type: tmpfs
      device: tmpfs